{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import clip\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load models\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def generate_cloak(image_pil, epsilon=0.003):\n",
    "    # Resize and convert to tensor\n",
    "    transform_to_tensor = T.Compose([\n",
    "        T.Resize((224, 224)),      # Required for CLIP\n",
    "        T.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = transform_to_tensor(image_pil).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Clone and normalize with grad enabled\n",
    "    image_clip = image_tensor.clone().detach().requires_grad_(True)\n",
    "    normalize_clip = T.Normalize(mean=(0.4815, 0.4578, 0.4082), std=(0.2686, 0.2613, 0.2758))\n",
    "    image_clip_norm = normalize_clip(image_clip)\n",
    "\n",
    "    # Text prompts\n",
    "    text_prompt = clip.tokenize([\"a painting of a cat\", \"an abstract landscape\"]).to(device)\n",
    "    text_features = clip_model.encode_text(text_prompt).mean(dim=0, keepdim=True)\n",
    "\n",
    "    # CLIP forward and loss\n",
    "    image_features = clip_model.encode_image(image_clip_norm)\n",
    "    clip_loss = -torch.cosine_similarity(image_features, text_features).mean()\n",
    "    clip_loss.backward()\n",
    "\n",
    "    # Cosine similarity of the original image and text (before perturbation)\n",
    "    original_image_features = clip_model.encode_image(image_clip_norm)\n",
    "    original_image_features = original_image_features / original_image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    original_cosine_similarity_score = torch.cosine_similarity(original_image_features, text_features).item()\n",
    "    print(f\"Original Cosine Similarity Score: {original_cosine_similarity_score}\")\n",
    "\n",
    "    # FGSM perturbation\n",
    "    perturbed = image_clip + epsilon * image_clip.grad.data.sign()\n",
    "    perturbed = torch.clamp(perturbed, 0, 1)\n",
    "\n",
    "    # Cosine similarity of the perturbed image and text\n",
    "    perturbed_image_features = clip_model.encode_image(normalize_clip(perturbed))\n",
    "    perturbed_image_features = perturbed_image_features / perturbed_image_features.norm(dim=-1, keepdim=True)\n",
    "    cosine_similarity_score = torch.cosine_similarity(perturbed_image_features, original_image_features).item()\n",
    "    print(f\"Perturbed Cosine Similarity Score: {cosine_similarity_score}\")\n",
    "    \n",
    "    embeddings = [perturbed_image_features, original_image_features]\n",
    "    reduced = TSNE(n_components=2).fit_transform(embeddings.cpu().numpy())\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1])\n",
    "    # Convert back to PIL\n",
    "    cloaked_img = T.ToPILImage()(perturbed.squeeze().detach().cpu())\n",
    "    return cloaked_img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Cosine Similarity Score: 0.188808873295784\n",
      "Perturbed Cosine Similarity Score: 0.8751435279846191\n",
      "Cloaked image saved to cloaked_output/art_cloaked.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Import the generate_cloak function from the module (assuming it's in cloak.py)\n",
    "# from cloak import generate_cloak\n",
    "\n",
    "# Paste the generate_cloak function code here if not using as a module.\n",
    "\n",
    "def cloak_and_save(input_path, output_path):\n",
    "    # Load input image\n",
    "    image = Image.open(input_path).convert(\"RGB\")\n",
    "    \n",
    "    # Cloak the image\n",
    "    cloaked_image = generate_cloak(image)\n",
    "    \n",
    "    # Save the cloaked image\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    cloaked_image.save(output_path)\n",
    "\n",
    "    print(f\"Cloaked image saved to {output_path}\")\n",
    "\n",
    "# === Example ===\n",
    "if __name__ == \"__main__\":\n",
    "    input_art_path = \"image.png\"      # Your input image path\n",
    "    output_cloak_path = \"cloaked_output/art_cloaked.jpg\"  # Output path\n",
    "\n",
    "    cloak_and_save(input_art_path, output_cloak_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embeddings: [original_embedding, cloaked_embedding, ...reference_embeddings]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ven1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
